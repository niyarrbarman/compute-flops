# -*- coding: utf-8 -*-
"""flops.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OzOYrJReEFXtxd_55sswOr37Y0i0z3Bx

## Calc FLOPs
"""

from transformers import AutoConfig
import math

# -----------------------------
#  Utility + Config structure
# -----------------------------
class LLMConfig:
    def __init__(self, model_name):
        cfg = AutoConfig.from_pretrained(model_name)
        self.model_name = model_name
        self.d_model = getattr(cfg, "hidden_size", getattr(cfg, "n_embd", None))
        self.n_layers = getattr(cfg, "num_hidden_layers", getattr(cfg, "n_layer", None))
        self.n_heads = getattr(cfg, "num_attention_heads", getattr(cfg, "n_head", None))
        self.d_ff = getattr(cfg, "intermediate_size", getattr(cfg, "n_inner", 4 * self.d_model))
        self.vocab_size = getattr(cfg, "vocab_size", None)
        self.tie = getattr(cfg, "tie_word_embeddings", False)
        self.num_kv_heads = getattr(cfg, "num_key_value_heads", self.n_heads)
        if any(v is None for v in [self.d_model, self.n_layers, self.n_heads, self.d_ff, self.vocab_size]):
            raise ValueError(f"Cannot infer dimensions for {model_name}")
        print(f"âœ… Loaded config for {model_name}")
        print(f"  Layers={self.n_layers}, d_model={self.d_model}, heads={self.n_heads}, d_ff={self.d_ff}, vocab={self.vocab_size}")

def flops_linear(in_dim, out_dim, tokens): return 2 * tokens * in_dim * out_dim
def flops_attn_scores(b,s,h,dh): return 2 * b * h * s * s * dh
def flops_attn_weighted_sum(b,s,h,dh): return 2 * b * h * s * s * dh
def flops_layernorm(tokens, d): return 8 * tokens * d
def flops_activation(tokens, d): return 4 * tokens * d  # GELU etc.

def humanize(n):
    for unit in ["", "K", "M", "G", "T", "P"]:
        if n < 1000: return f"{n:.3f}{unit}"
        n /= 1000
    return f"{n:.3f}E"

# ----------------------------------------
#  FLOPs estimation for decoder-only block
# ----------------------------------------
def decoder_block_flops(cfg, batch, seq, kv_cache=False, mode="inference", gated_ffn=True):
    H = cfg.d_model
    Nh = cfg.n_heads
    Dh = H // Nh
    tokens = batch * seq

    # --- Attention ---
    qkv = 3 * flops_linear(H, H, tokens)
    if kv_cache and mode == "inference":
        tri = seq * (seq + 1) // 2
        scores = 2 * batch * Nh * Dh * tri
        av = 2 * batch * Nh * Dh * tri
    else:
        scores = flops_attn_scores(batch, seq, Nh, Dh)
        av = flops_attn_weighted_sum(batch, seq, Nh, Dh)
    attn_out = flops_linear(H, H, tokens)

    # --- MLP (SwiGLU default) ---
    if gated_ffn:
        up = 2 * flops_linear(H, cfg.d_ff, tokens)  # two projections
        act = flops_activation(tokens, cfg.d_ff)
        gate = tokens * cfg.d_ff
    else:
        up = flops_linear(H, cfg.d_ff, tokens)
        act = flops_activation(tokens, cfg.d_ff)
        gate = 0
    down = flops_linear(cfg.d_ff, H, tokens)

    # --- LayerNorms ---
    ln = 2 * flops_layernorm(tokens, H)

    fwd = qkv + scores + av + attn_out + up + act + gate + down + ln
    if mode == "training": fwd *= 3
    return fwd

#  Total FLOPs for all (remaining) layers

def compute_llm_flops(model_name, batch=1, seq=2048, mode="inference",
                      kv_cache=True, drop_layers=None, gated_ffn=True,
                      include_lm_head=True, one_based=True):
    cfg = LLMConfig(model_name)
    drop_layers = drop_layers or []
    drop = set([(i - 1) if one_based else i for i in drop_layers])
    kept = cfg.n_layers - len(drop)

    per_block = decoder_block_flops(cfg, batch, seq, kv_cache, mode, gated_ffn)
    total_blocks = kept * per_block
    lm_head = 2 * batch * seq * cfg.d_model * cfg.vocab_size if include_lm_head else 0
    total = total_blocks + lm_head

    print(f"\nðŸ“Š FLOPs summary for {model_name}")
    print(f"Layers: {cfg.n_layers} (kept {kept}, dropped {sorted(drop_layers)})")
    print(f"Batch={batch}, Seq={seq}, Mode={mode}, KV-cache={kv_cache}, Gated-FFN={gated_ffn}")
    print(f"Include LM Head={include_lm_head}")
    print(f"Total â‰ˆ {humanize(total)} FLOPs ({total:,})")
    print(f"  Per block â‰ˆ {humanize(per_block)} FLOPs")
    return total

"""## Test"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=8096,
    mode="inference",
    kv_cache=True,
    drop_layers=[],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=8096,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""## ARC-Challenge

#### Dataset
"""

# dataset_prep.py
from datasets import load_dataset
import csv
import os

def prepare_dataset(split, output_csv):
    # Load the specified split of ARC-Easy from HF
    #dataset = load_dataset("allenai/ai2_arc", "ARC-Easy", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")
    dataset = load_dataset("allenai/ai2_arc", "ARC-Challenge", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")

    with open(output_csv, "w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["question", "choice_text", "choice_label", "answer_key"])

        for item in dataset:
            question = item["question"]
            answer_key = item["answerKey"]
            for choice in item["choices"]["text"]:
                idx = item["choices"]["text"].index(choice)
                label = item["choices"]["label"][idx]
                writer.writerow([question, choice, label, answer_key])

    print(f"CSV for {split} saved as {output_csv}")

# Prepare train and validation CSVs
#prepare_dataset("train", "/tmpdir/m24047nmmr/pruning/datasets/arc/arc_easy_train.csv")
prepare_dataset("validation", "arc_easy_validation.csv")
# prepare_dataset("train", "/tmpdir/m24047nmmr/pruning/datasets/arc/arc_challenge_train.csv")

"""#### Model"""

from transformers import AutoTokenizer
import pandas as pd

# --- Load tokenizer ---
model_name = "meta-llama/Meta-Llama-3.1-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# --- Load your dataset ---
df = pd.read_csv("/content/arc_easy_validation.csv")

# Check which column holds the question
print("Columns:", df.columns)
# e.g., it might be 'question', 'Question', or 'prompt'
col = [c for c in df.columns if 'question' in c.lower()][0]
print("Using column:", col)

# --- System prompt (exact as given) ---
system_prompt = (
    "You are a Science expert assistant. "
    "Your task is to answer multiple-choice science questions at grade-school level. "
    "Each question has four answer choices, labeled A, B, C, and D. "
    "For each question:\n"
    "- Carefully read the question and all answer choices.\n"
    "- Select the single best answer from the options (A, B, C, or D).\n"
    "- Respond only with the letter of the correct answer, and nothing elseâ€”no explanation or extra words.\n"
    "Be precise and consistent: Only the answer letter."
)

# --- Tokenization for each question ---
prompt_tokens = []
for q in df[col]:
    full_prompt = system_prompt + "\n\nQuestion: " + str(q)
    ids = tokenizer(full_prompt, add_special_tokens=True)["input_ids"]
    prompt_tokens.append(len(ids))

# --- Compute stats ---
total_tokens = sum(prompt_tokens) + 1*len(prompt_tokens)
avg_tokens = total_tokens / len(prompt_tokens)
max_tokens = max(prompt_tokens)
min_tokens = min(prompt_tokens)

print(f"ðŸ“Š Token statistics (LLaMA-3.1-8B tokenizer):")
print(f"  Total prompts: {len(prompt_tokens)}")
print(f"  Total tokens: {total_tokens:,}")
print(f"  Avg tokens per prompt: {avg_tokens:.2f}")
print(f"  Min tokens: {min_tokens}, Max tokens: {max_tokens}")
print(f"  Max new tokens (model setting): 1")

"""#### Baseline"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=133.52,
    mode="inference",
    kv_cache=True,
    drop_layers=[],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### Best Model"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=133.52,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 22, 26],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### BSBA"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=133.52,
    mode="inference",
    kv_cache=True,
    drop_layers=[18,19,20,22,24,26,27],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""## ARC-Easy"""

# dataset_prep.py
from datasets import load_dataset
import csv
import os

def prepare_dataset(split, output_csv):
    # Load the specified split of ARC-Easy from HF
    dataset = load_dataset("allenai/ai2_arc", "ARC-Easy", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")
    # dataset = load_dataset("allenai/ai2_arc", "ARC-Challenge", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")

    with open(output_csv, "w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["question", "choice_text", "choice_label", "answer_key"])

        for item in dataset:
            question = item["question"]
            answer_key = item["answerKey"]
            for choice in item["choices"]["text"]:
                idx = item["choices"]["text"].index(choice)
                label = item["choices"]["label"][idx]
                writer.writerow([question, choice, label, answer_key])

    print(f"CSV for {split} saved as {output_csv}")

# Prepare train and validation CSVs
#prepare_dataset("train", "/tmpdir/m24047nmmr/pruning/datasets/arc/arc_easy_train.csv")
prepare_dataset("validation", "arc_easy_validation.csv")
# prepare_dataset("train", "/tmpdir/m24047nmmr/pruning/datasets/arc/arc_challenge_train.csv")

from transformers import AutoTokenizer
import pandas as pd

# --- Load tokenizer ---
model_name = "meta-llama/Meta-Llama-3.1-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# --- Load your dataset ---
df = pd.read_csv("/content/arc_easy_validation.csv")

# Check which column holds the question
print("Columns:", df.columns)
# e.g., it might be 'question', 'Question', or 'prompt'
col = [c for c in df.columns if 'question' in c.lower()][0]
print("Using column:", col)

# --- System prompt (exact as given) ---
system_prompt = (
    "You are a Science expert assistant. "
    "Your task is to answer multiple-choice science questions at grade-school level. "
    "Each question has four answer choices, labeled A, B, C, and D. "
    "For each question:\n"
    "- Carefully read the question and all answer choices.\n"
    "- Select the single best answer from the options (A, B, C, or D).\n"
    "- Respond only with the letter of the correct answer, and nothing elseâ€”no explanation or extra words.\n"
    "Be precise and consistent: Only the answer letter."
)

# --- Tokenization for each question ---
prompt_tokens = []
for q in df[col]:
    full_prompt = system_prompt + "\n\nQuestion: " + str(q)
    ids = tokenizer(full_prompt, add_special_tokens=True)["input_ids"]
    prompt_tokens.append(len(ids))

# --- Compute stats ---
total_tokens = sum(prompt_tokens) + 1*len(prompt_tokens)
avg_tokens = total_tokens / len(prompt_tokens)
max_tokens = max(prompt_tokens)
min_tokens = min(prompt_tokens)

print(f"ðŸ“Š Token statistics (LLaMA-3.1-8B tokenizer):")
print(f"  Total prompts: {len(prompt_tokens)}")
print(f"  Total tokens: {total_tokens:,}")
print(f"  Avg tokens per prompt: {avg_tokens:.2f}")
print(f"  Min tokens: {min_tokens}, Max tokens: {max_tokens}")
print(f"  Max new tokens (model setting): 1")



"""#### Baseline"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=129.53,
    mode="inference",
    kv_cache=True,
    drop_layers=[],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### Best Model"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=129.53,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### BSBA"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=129.53,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 21, 24, 26, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""## Bool-Q"""

from datasets import load_dataset
import csv
import os

def prepare_boolq_dataset(split, output_csv):
    # Load the specified split of BoolQ from HF
    dataset = load_dataset("google/boolq", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")

    with open(output_csv, "w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        # BoolQ has different structure - question, passage, and boolean answer
        writer.writerow(["question", "passage", "answer", "answer_label"])

        for item in dataset:
            question = item["question"]
            passage = item["passage"]
            answer = item["answer"]  # This is boolean (True/False)

            # Convert boolean to standard format
            answer_label = "A" if answer else "B"  # True = A, False = B
            answer_text = "True" if answer else "False"

            writer.writerow([question, passage, answer_text, answer_label])

    print(f"CSV for {split} saved as {output_csv}")
    print(f"Total samples in {split}: {len(dataset)}")

prepare_boolq_dataset("validation", "boolq_validation.csv")

from transformers import AutoTokenizer
import pandas as pd

# --- Load tokenizer ---
model_name = "meta-llama/Meta-Llama-3.1-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# --- Load your dataset ---
df = pd.read_csv("/content/boolq_validation.csv")

# Check which column holds the question
print("Columns:", df.columns)
# e.g., it might be 'question', 'Question', or 'prompt'
col = [c for c in df.columns if 'question' in c.lower()][0]
print("Using column:", col)

# --- System prompt (exact as given) ---
system_prompt = (
                "You are a helpful assistant that answers True/False questions based on given passages. "
                "Read the passage carefully and determine if the question can be answered as True or False based on the information in the passage. "
                "Respond with only 'A' for True or 'B' for False."
            )

# --- Tokenization for each question ---
prompt_tokens = []
for q in df[col]:
    full_prompt = system_prompt + "\n\nQuestion: " + str(q)
    ids = tokenizer(full_prompt, add_special_tokens=True)["input_ids"]
    prompt_tokens.append(len(ids))

# --- Compute stats ---
total_tokens = sum(prompt_tokens) + 1*len(prompt_tokens)
avg_tokens = total_tokens / len(prompt_tokens)
max_tokens = max(prompt_tokens)
min_tokens = min(prompt_tokens)

print(f"ðŸ“Š Token statistics (LLaMA-3.1-8B tokenizer):")
print(f"  Total prompts: {len(prompt_tokens)}")
print(f"  Total tokens: {total_tokens:,}")
print(f"  Avg tokens per prompt: {avg_tokens:.2f}")
print(f"  Min tokens: {min_tokens}, Max tokens: {max_tokens}")
print(f"  Max new tokens (model setting): 1")



"""#### Baseline"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### Best Model"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### BSBA"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 21, 24, 26, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""## Bool-Q"""

from datasets import load_dataset
import csv
import os

def prepare_boolq_dataset(split, output_csv):
    # Load the specified split of BoolQ from HF
    dataset = load_dataset("google/boolq", split=split, cache_dir="/tmpdir/m24047nmmr/pruning/datasets/cache")

    with open(output_csv, "w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        # BoolQ has different structure - question, passage, and boolean answer
        writer.writerow(["question", "passage", "answer", "answer_label"])

        for item in dataset:
            question = item["question"]
            passage = item["passage"]
            answer = item["answer"]  # This is boolean (True/False)

            # Convert boolean to standard format
            answer_label = "A" if answer else "B"  # True = A, False = B
            answer_text = "True" if answer else "False"

            writer.writerow([question, passage, answer_text, answer_label])

    print(f"CSV for {split} saved as {output_csv}")
    print(f"Total samples in {split}: {len(dataset)}")

prepare_boolq_dataset("validation", "boolq_validation.csv")

from transformers import AutoTokenizer
import pandas as pd

# --- Load tokenizer ---
model_name = "meta-llama/Meta-Llama-3.1-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# --- Load your dataset ---
df = pd.read_csv("/content/boolq_validation.csv")

# Check which column holds the question
print("Columns:", df.columns)
# e.g., it might be 'question', 'Question', or 'prompt'
col = [c for c in df.columns if 'question' in c.lower()][0]
print("Using column:", col)

# --- System prompt (exact as given) ---
system_prompt = (
                "You are a helpful assistant that answers True/False questions based on given passages. "
                "Read the passage carefully and determine if the question can be answered as True or False based on the information in the passage. "
                "Respond with only 'A' for True or 'B' for False."
            )

# --- Tokenization for each question ---
prompt_tokens = []
for q in df[col]:
    full_prompt = system_prompt + "\n\nQuestion: " + str(q)
    ids = tokenizer(full_prompt, add_special_tokens=True)["input_ids"]
    prompt_tokens.append(len(ids))

# --- Compute stats ---
total_tokens = sum(prompt_tokens) + 1*len(prompt_tokens)
avg_tokens = total_tokens / len(prompt_tokens)
max_tokens = max(prompt_tokens)
min_tokens = min(prompt_tokens)

print(f"ðŸ“Š Token statistics (LLaMA-3.1-8B tokenizer):")
print(f"  Total prompts: {len(prompt_tokens)}")
print(f"  Total tokens: {total_tokens:,}")
print(f"  Avg tokens per prompt: {avg_tokens:.2f}")
print(f"  Min tokens: {min_tokens}, Max tokens: {max_tokens}")
print(f"  Max new tokens (model setting): 1")



"""#### Baseline"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### Best Model"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)

"""#### BSBA"""

compute_llm_flops(
    model_name="meta-llama/Llama-3.1-8B",
    batch=1,
    seq=68.71,
    mode="inference",
    kv_cache=True,
    drop_layers=[18, 19, 20, 21, 24, 26, 28, 31],
    one_based=True,
    gated_ffn=True,
    include_lm_head=True
)